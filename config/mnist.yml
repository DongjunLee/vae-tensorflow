model:
  batch_size: 32

train:
  learning_rate: 0.0001
  optimizer: 'Adam'                # Adagrad, Adam, Ftrl, Momentum, RMSProp, SGD

  train_steps: 100000
  model_dir: 'logs/mnist'

  save_checkpoints_steps: 1000
  check_hook_n_iter: 1000
  min_eval_frequency: 10

  print_verbose: True
  debug: False

slack:
  webhook_url: ""                   # after training notify you using slack-webhook
